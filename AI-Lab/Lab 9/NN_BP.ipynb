{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "394d9d93-2e5c-4419-b48b-5dc682c2c37b",
    "_uuid": "b4d887deb330df3f06decde04734ca0335358f23"
   },
   "source": [
    "# Neural Network With Back Propogation\n",
    "\n",
    "# Content \n",
    "1. Introduction\n",
    "2. Error back-propagation algorithm theory\n",
    "3. Database description\n",
    "4. Back-propagation Neural Network implementation\n",
    "5. Conclusion\n",
    "***\n",
    "## 1. Introduction\n",
    "\n",
    "The Back-Propagation Neural Network is a feed-forward network with a quite simple arhitecture. The arhitecture of the network consists of an input layer, one or more hidden layers and an output layer. This type of network can distinguish data that is not linearly separable. We use [error back-propagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm to tune the network iterative. \n",
    "***\n",
    " ## 2. Error back-propagation algorithm theory\n",
    " \n",
    "The error back-propagation algorithm consists of two big steps:\n",
    "1. Feeding forward the input from the database to the input layer than to the hidden layers and finally to the output layer.\n",
    "2. Calculating the output error and feeding it backwards tuning the network's variables.\n",
    "***\n",
    "## 3. Database description\n",
    "\n",
    "In this example we are going to use Duke Breast Cancer database that consists of [86] entries and [7129] attributes plus the class attribute that is located on the first column.\n",
    "The data is numerical and has no missing values.\n",
    "***\n",
    "## 4. Back-propagation Neural Network implementation\n",
    "\n",
    "First of all, we need to load the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "db = np.loadtxt(\"duke-breast-cancer.txt\")\n",
    "print(\"Database raw shape (%s,%s)\" % np.shape(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "Now we have to shuffle it and then split it into training 90%  and testing 10% so that the network can train itslef better. If needed you can also normalize the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "24bea183-2074-4d53-a47a-cf05bb5ec7ed",
    "_uuid": "762d37c4dae44319c9d2422e169d7ea0f33572a9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(db)\n",
    "y = db[:, 0]\n",
    "x = np.delete(db, [0], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "print(np.shape(x_train),np.shape(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1339c51-6710-4bf3-89f9-d4a58f1efead",
    "_uuid": "22981874e171b499eb46ef87b768e9cd02f67d68"
   },
   "source": [
    "Now we have  to create the hidden layer vector, weight's matrix,  output layer vector and the hidden weight's matrix. We choose the hidden layer to be made of a number of [72] hidden perceptrons. The output layer needs to have a number of perceptrons equal to the number of classes.The weight's matrix will have the following shape: lines = number of the database attributes, cols = number of hidden layer perceptrons and the hidden weight's matrix will have the following shape: lines = hidden layer length, cols = number of output layer perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c56476bc-9ed6-4f59-adcc-05c1d7efffe8",
    "_uuid": "c6e92169cd8d40cfbd6535d89ef9d568ec7f080a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hidden_layer = np.zeros(72)\n",
    "weights = np.random.random((len(x[0]), 72))\n",
    "output_layer = np.zeros(2)\n",
    "hidden_weights = np.random.random((72, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f0f46d5-6522-4993-9cec-5e3ce2f04b04",
    "_uuid": "9e5dedcada121685d8b54cc88d9b367d824edb1c"
   },
   "source": [
    "To continue we need to implement: \n",
    "1. Sum function\n",
    "2. Activation function\n",
    "3. SoftMax function\n",
    "4. Recalculate Weights function\n",
    "5. Back-propagation function\n",
    "\n",
    "## Sum function\n",
    "s_i is the sum for [i]th perceptron from the layer.\n",
    "![sum.png](https://image.ibb.co/i3EM27/sum.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "64eef7a8-6c64-41c9-857b-8719e740acb9",
    "_uuid": "a9363b0d1930d5ab7bf109087c44f9f1f5378e04",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sum_function(weights, index_locked_col, x):\n",
    "    result = 0\n",
    "    for i in range(0, len(x)):\n",
    "        result += x[i] * weights[i][index_locked_col]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cc370a5d-bfe2-406d-8543-35b6558dc728",
    "_uuid": "56acfc5f17e7105d3cd4f2a4896dca97a7a48271"
   },
   "source": [
    "## Activation function\n",
    "g(s_i) is the activation for the [i]th perceptron from the layer.\n",
    "![image.png](https://image.ibb.co/eTfYFS/g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6c9963c8-0c5b-4b5d-8254-940017be5147",
    "_uuid": "4d8da27a9d20a0d9e22ccbc7f2e97a2ad1ce3d8d",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def activate_layer(layer, weights, x):\n",
    "    for i in range(0, len(layer)):\n",
    "        layer[i] = 1.7159 * np.tanh(2.0 * sum_function(weights, i, x) / 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "909ef5c7-7017-4d03-943a-56eff9b91cd6",
    "_uuid": "a9824a7d2937bf0435e48419e7bc07d6803da551"
   },
   "source": [
    "## SoftMax function\n",
    "[The softmax function, or normalized exponential function, is a generalization of the logistic function that \"squashes\" a K-dimensional vector z of arbitrary real values to a K-dimensional vector Ïƒ ( z ) of real values in the range (0, 1) that add up to 1.](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "![image.png](https://image.ibb.co/dKsr27/sm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c33b200b-07e1-475a-9310-4b6486f36f46",
    "_uuid": "cd1e82c77d052992d5b8bfe9ea4793c7dae9fa82",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def soft_max(layer):\n",
    "    soft_max_output_layer = np.zeros(len(layer))\n",
    "    for i in range(0, len(layer)):\n",
    "        denominator = 0\n",
    "        for j in range(0, len(layer)):\n",
    "            denominator += np.exp(layer[j] - np.max(layer))\n",
    "        soft_max_output_layer[i] = np.exp(layer[i] - np.max(layer)) / denominator\n",
    "    return soft_max_output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4c83acf-fadf-4197-a8c3-f597fd487b3c",
    "_uuid": "f557cad437c5de3f5a4628d0efebbe0d8b5c84da"
   },
   "source": [
    "## Recalculate weights function\n",
    "Here we tune the network weights and hidden weights matrix. We are going to use this inside the back propagation function.\n",
    "![image.png](https://image.ibb.co/moBepn/w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ee9f8b5-35a4-4a20-85b6-e13fc2c16db8",
    "_uuid": "a2d58312392a090b6f9c07b4c251bfae866aaefa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def recalculate_weights(learning_rate, weights, gradient, activation):\n",
    "    for i in range(0, len(weights)):\n",
    "        for j in range(0, len(weights[i])):\n",
    "            weights[i][j] = (learning_rate * gradient[j] * activation[i]) + weights[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "406650e7-9042-4718-91f7-7069deb4f395",
    "_uuid": "f0cbc7cdcd6412f74ba12921a20817ce5d9d51cf"
   },
   "source": [
    "## Back-propagation function\n",
    "In this function we find out the output layer gradient and the hidden layer gradient to recalculate the network weights.\n",
    "Output gradient formula\n",
    "![image.png](https://image.ibb.co/eJ9qUn/go.png)\n",
    "Hidden gradient formula\n",
    "![image.png](https://image.ibb.co/mYQ3h7/gh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "886bcb3a-4b24-4d2c-bb93-a932a0e5e5bf",
    "_uuid": "bf147c36b5709607dad90ab78749a77dea18f0d6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation(hidden_layer, output_layer, one_hot_encoding, learning_rate, x):\n",
    "    output_derivative = np.zeros(2)\n",
    "    output_gradient = np.zeros(2)\n",
    "    for i in range(0, len(output_layer)):\n",
    "        output_derivative[i] = (1.0 - output_layer[i]) * output_layer[i]\n",
    "    for i in range(0, len(output_layer)):\n",
    "        output_gradient[i] = output_derivative[i] * (one_hot_encoding[i] - output_layer[i])\n",
    "    hidden_derivative = np.zeros(72)\n",
    "    hidden_gradient = np.zeros(72)\n",
    "    for i in range(0, len(hidden_layer)):\n",
    "        hidden_derivative[i] = (1.0 - hidden_layer[i]) * (1.0 + hidden_layer[i])\n",
    "    for i in range(0, len(hidden_layer)):\n",
    "        sum_ = 0\n",
    "        for j in range(0, len(output_gradient)):\n",
    "            sum_ += output_gradient[j] * hidden_weights[i][j]\n",
    "        hidden_gradient[i] = sum_ * hidden_derivative[i]\n",
    "    recalculate_weights(learning_rate, hidden_weights, output_gradient, hidden_layer)\n",
    "    recalculate_weights(learning_rate, weights, hidden_gradient, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d84f6360-946c-41a8-aeb0-15c07ddff96e",
    "_uuid": "3d8e2c2b5da99f9b0e1abb19b9ba7dcefb26e518"
   },
   "source": [
    "Next we can [one hot encode](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science) our output and start training our network iterative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e2dbc9d-0f08-4c0b-94d7-31a4b90dc46c",
    "_kg_hide-input": false,
    "_uuid": "842e987aa6eeab97f21fceb01c49d1465983e9cd",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "one_hot_encoding = np.zeros((2,2))\n",
    "for i in range(0, len(one_hot_encoding)):\n",
    "    one_hot_encoding[i][i] = 1\n",
    "training_correct_answers = 0\n",
    "for i in range(0, len(x_train)):\n",
    "    activate_layer(hidden_layer, weights, x_train[i])\n",
    "    activate_layer(output_layer, hidden_weights, hidden_layer)\n",
    "    output_layer = soft_max(output_layer)\n",
    "    training_correct_answers += 1 if y_train[i] == np.argmax(output_layer) else 0\n",
    "    back_propagation(hidden_layer, output_layer, one_hot_encoding[int(y_train[i])], -1, x_train[i])\n",
    "print(\"MLP Correct answers while learning: %s / %s (Accuracy = %s) on %s database.\" % (training_correct_answers, len(x_train), \n",
    "                                                                                       training_correct_answers/len(x_train),\"Duke breast cancer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfd53b22-e2fa-4dc6-9dfc-08ebd25355a4",
    "_uuid": "a07b936e15e43679d208ff3a2dab306ba861f11b"
   },
   "source": [
    "The accuracy of the test depends on the random generated weight's matrix and the learning rate. Using different learning rates and weight's will generate a different accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e97419b-e5b8-4751-b1aa-b92adbdb7883",
    "_kg_hide-output": false,
    "_uuid": "d054f30b2e1514d48825906f72a667ae139dddda",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "testing_correct_answers = 0\n",
    "for i in range(0, len(x_test)):\n",
    "    activate_layer(hidden_layer, weights, x_test[i])\n",
    "    activate_layer(output_layer, hidden_weights, hidden_layer)\n",
    "    output_layer = soft_max(output_layer)\n",
    "    testing_correct_answers += 1 if y_test[i] == np.argmax(output_layer) else 0\n",
    "print(\"MLP Correct answers while testing: %s / %s (Accuracy = %s) on %s database\" % (testing_correct_answers, len(x_test),\n",
    "                                                                                     testing_correct_answers/len(x_test), \"Duke breast cancer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb49e27e-f3f6-4743-b8ea-fb3634782a4e",
    "_uuid": "a1052057b9c6ab253b9758cbc8cc30707440b90d"
   },
   "source": [
    "On this testing set the accuracy can go up to even 100%  with the right amount of hidden perceptrons in the hidden layer. In this example we used a learning rate of [-1] with a total of [72] hidden perceptrons in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96a1f6e2-6f22-4717-a614-590ee0b7776d",
    "_uuid": "aaf9f269d3c7400a9d9ac82ebdc1a960d0792016"
   },
   "source": [
    "***\n",
    "## 5.  Conclusion\n",
    "In this test we have shown that the back-propagation neural network performs well on large sets of data. The performance can be improved by changing the number of hidden neurons and the learning rate.\n",
    "Because of its iterative training and gradient based training the general speed is far slower than required, so it takes a large amount of time to train on a very large set of data. We cannot say that there is a perfect network for every kind of database out there. So keep testing your data on multiple neural networks and see what fits the best.\n",
    "\n",
    "I hope this notebook helped you to begin your journey into machine learning and big data world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
